{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "Converted to torch object torch.LongTensor\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "Converted to numpy int64\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "#convert numpy to tensor or vice versa\n",
    "X_original=np.array([[1,2,3,4],[5,6,7,8]])\n",
    "print('Original Data')\n",
    "print(X_original)\n",
    "X_torch = torch.from_numpy(X_original)\n",
    "print('Converted to torch object', X_torch.type())\n",
    "print(X_torch)\n",
    "X_numpy = X_torch.numpy()\n",
    "print('Converted to numpy', X_numpy.dtype)\n",
    "print(X_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert torch object to a float tensor object\n",
      "torch.FloatTensor\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.]])\n",
      "sin\n",
      "tensor([[ 0.8415,  0.9093,  0.1411, -0.7568],\n",
      "        [-0.9589, -0.2794,  0.6570,  0.9894]])\n",
      "exp\n",
      "tensor([[2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01],\n",
      "        [1.4841e+02, 4.0343e+02, 1.0966e+03, 2.9810e+03]])\n",
      "abs\n",
      "tensor([[0.8415, 0.9093, 0.1411, 0.7568],\n",
      "        [0.9589, 0.2794, 0.6570, 0.9894]])\n",
      "sigmoid\n",
      "tensor([[0.7311, 0.8808, 0.9526, 0.9820],\n",
      "        [0.9933, 0.9975, 0.9991, 0.9997]])\n",
      "mean\n",
      "tensor(4.5000)\n"
     ]
    }
   ],
   "source": [
    "#Math with PyTorch objects\n",
    "print('Convert torch object to a float tensor object')\n",
    "X_torch = X_torch.float()\n",
    "print(X_torch.type())\n",
    "print(X_torch)\n",
    "print('sin')\n",
    "print(torch.sin(X_torch))\n",
    "print('exp')\n",
    "print(X_torch.exp())\n",
    "print('abs')\n",
    "print(torch.sin(X_torch).abs())\n",
    "print('sigmoid')\n",
    "print(X_torch.sigmoid())\n",
    "print('mean')\n",
    "print(torch.mean(X_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert numpy array to a float numpy array\n",
      "float64\n",
      "sin\n",
      "[[ 0.84147098  0.90929743  0.14112001 -0.7568025 ]\n",
      " [-0.95892427 -0.2794155   0.6569866   0.98935825]]\n",
      "exp\n",
      "[[2.71828183e+00 7.38905610e+00 2.00855369e+01 5.45981500e+01]\n",
      " [1.48413159e+02 4.03428793e+02 1.09663316e+03 2.98095799e+03]]\n",
      "abs\n",
      "[[0.84147098 0.90929743 0.14112001 0.7568025 ]\n",
      " [0.95892427 0.2794155  0.6569866  0.98935825]]\n",
      "mean\n",
      "4.5\n"
     ]
    }
   ],
   "source": [
    "#In contrast, math with numpy\n",
    "print('Convert numpy array to a float numpy array')\n",
    "X_numpy = X_numpy.astype(float)\n",
    "print(X_numpy.dtype)\n",
    "print('sin')\n",
    "print(np.sin(X_numpy))\n",
    "print('exp')\n",
    "print(np.exp(X_numpy))\n",
    "print('abs')\n",
    "print(np.abs(np.sin(X_numpy)))\n",
    "print('mean')\n",
    "print(np.mean(X_numpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00, -2.5244e-29, -1.5042e+31],\n",
      "        [-2.0005e+00,  5.6052e-45,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -2.5244e-29, -1.5019e+31]])\n",
      "tensor([[0.1408, 0.6017, 0.9808],\n",
      "        [0.9822, 0.1319, 0.0996],\n",
      "        [0.0726, 0.7683, 0.2835],\n",
      "        [0.7106, 0.3597, 0.7049],\n",
      "        [0.1850, 0.7154, 0.4848]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "tensor([[2.3000, 6.4000, 8.5000],\n",
      "        [1.1000, 4.4000, 3.0000]])\n"
     ]
    }
   ],
   "source": [
    "#making torch tensors\n",
    "#empty tensor\n",
    "A = torch.empty(5,3)\n",
    "print(A) \n",
    "#filled with random numbers\n",
    "A = torch.rand(5,3)\n",
    "print(A)\n",
    "#filled with zeros\n",
    "A=torch.zeros(5,3, dtype=torch.long)\n",
    "print(A)\n",
    "#filled with ones\n",
    "A=torch.ones(5,3, dtype=torch.long)\n",
    "print(A)\n",
    "#making a tensor from data\n",
    "A = torch.tensor([[2.3,6.4,8.5],[1.1,4.4,3.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of A: torch.Size([2, 3])\n",
      "resize tensors\n",
      "tensor([[2.3000, 6.4000],\n",
      "        [8.5000, 1.1000],\n",
      "        [4.4000, 3.0000]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "#operations with torch tensors\n",
    "#get size, it'll be a torch size object\n",
    "print('size of A:', A.size())\n",
    "#resize tensors\n",
    "print('resize tensors')\n",
    "A=A.view(3,2)\n",
    "print(A)\n",
    "print(A.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add tensors\n",
      "tensor([[2.6275, 7.3160],\n",
      "        [8.5987, 1.1222],\n",
      "        [4.8443, 3.5318]])\n",
      "tensor([[2.6275, 7.3160],\n",
      "        [8.5987, 1.1222],\n",
      "        [4.8443, 3.5318]])\n"
     ]
    }
   ],
   "source": [
    "#add tensors\n",
    "print('Add tensors')\n",
    "B=torch.rand(A.size())\n",
    "print(A+B)\n",
    "# or use torch operations\n",
    "print(torch.add(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiply tensors\n",
      "numpy: [[ 7 10]\n",
      " [15 22]]\n",
      "torch: tensor([[ 7., 10.],\n",
      "        [15., 22.]])\n"
     ]
    }
   ],
   "source": [
    "#multiply tensors\n",
    "print('Multiply tensors')\n",
    "data = [[1,2], [3,4]]\n",
    "tensor = torch.FloatTensor(data)  # 32-bit floating point\n",
    "print('numpy:', np.matmul(data, data))\n",
    "print('torch:', torch.mm(tensor, tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.299999952316284\n"
     ]
    }
   ],
   "source": [
    "#get value of 1-element tensor\n",
    "print(A[0,0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "D: [1. 1. 1.] C: tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#numpy bridge, torch tensor and numpy array share same memory location!\n",
    "C = torch.ones(3)\n",
    "print(C)\n",
    "D = C.numpy()\n",
    "print('D:', D, 'C:', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [1. 1. 4.] C: tensor([1., 1., 4.])\n"
     ]
    }
   ],
   "source": [
    "D[2]=4\n",
    "print('D:', D, 'C:', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: [1. 2. 4.] C: tensor([1., 2., 4.])\n"
     ]
    }
   ],
   "source": [
    "C[1]=2\n",
    "print('D:', D, 'C:', C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([[-0.7789, -0.3291, -0.0742, -0.2087,  0.2622, -1.2285],\n",
      "        [-0.5661, -1.1376, -0.0303, -0.6101,  0.6286, -0.1224],\n",
      "        [ 0.7302,  0.9954, -0.4946,  0.3608,  0.1927,  0.1650]])\n",
      "z.size() torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# can also concatenate two tensors together on various dimensions\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "z = torch.cat((x, y), 1)\n",
    "print(\"z:\", z)\n",
    "print(\"z.size()\", z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Neural Network on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torch.optim as optim\n",
    "import os \n",
    "import time\n",
    "from tqdm import tqdm  # this is a nice library for making nice looking progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer1 = nn.Linear(in_features=3, out_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1303, -0.3429,  0.1492,  0.4129,  1.0282,  0.5080],\n",
      "        [-0.2372,  0.2043, -0.2989,  0.4768,  0.8333,  0.1873],\n",
      "        [ 0.5576,  0.2275, -1.2040,  0.7007, -0.1425,  0.6203]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "x2 = linear_layer1(x)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5552,  0.5200, -0.0170],\n",
       "        [-0.1057, -0.2718,  0.5311],\n",
       "        [ 0.0287, -0.4084, -0.0815],\n",
       "        [-0.0317,  0.1445, -0.0781],\n",
       "        [-0.4319, -0.3663, -0.4280],\n",
       "        [ 0.5097,  0.2445,  0.1543]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1464,  0.4599, -0.5521,  0.4773,  0.2089,  0.5598],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST(root=\".\", download=True, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the size of the first sample in mnist\n",
    "mnist[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model as an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        define model attributes\n",
    "        \"\"\"\n",
    "        self.layer1 = nn.Linear(28*28, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        define model\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 28*28)\n",
    "        h1 = self.relu(self.layer1(x))\n",
    "        h2 = self.relu(self.layer2(h1))\n",
    "        logits = self.relu(self.layer3(h2))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to train model with mnist dataset (plain python code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, save_dir):\n",
    "        self.model = model\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def train(self, train, val, epochs, batch_size, log_per_batches, learning_rate, device):\n",
    "\n",
    "        run_id = time.clock()  # give this training run a unique id, we'll use this later when we save our model\n",
    "\n",
    "        # batch data and pass to iterator\n",
    "        trainloader = DataLoader(train, batch_size=batch_size, shuffle=True)  # pass training data to pytorch's data loader (an iterator)\n",
    "\n",
    "        if val is not None:\n",
    "            valloader = DataLoader(val, batch_size=batch_size, shuffle=True)  # pass validation data to pytorch's dataloader (an iterator)\n",
    "\n",
    "        # stuff for you to play with!\n",
    "        loss_fn = nn.CrossEntropyLoss()  # you will need to pick an appropriate loss function for the model you're building\n",
    "        optimizer = optim.Adam(self.model.parameters(), learning_rate)  # adam is typically the standard choice but feel free to play around w this\n",
    "        \n",
    "        # keep a running loss that we'll average over - this gives us a smoother estimate on the loss\n",
    "        running_train_loss = 0\n",
    "        running_val_loss = 0\n",
    "\n",
    "        # we'll also keep track of our previous loss on the validation set and use this figure out when to stop training \n",
    "        prev_val_loss = 0\n",
    "        \n",
    "        # begin train loop\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch %s\" % epoch)\n",
    "\n",
    "            train_bar = tqdm(enumerate(trainloader, 1), total=len(trainloader))  # wrap with tqdm to make a pretty progress bar\n",
    "\n",
    "            # iterate over training data\n",
    "            for i, data in train_bar:\n",
    "                x, targets = data\n",
    "\n",
    "                optimizer.zero_grad()  # call to zero gradients\n",
    "\n",
    "                predictions = self.model(x)  # pass x to model and return predictions\n",
    "                loss = loss_fn(predictions, targets)  # calculate loss on this batch of predictions\n",
    "                running_train_loss += loss  # add to our running total\n",
    "                loss.backward()  # backpropogate the loss and update model parameters\n",
    "                optimizer.step()  # update optimizer paraeters\n",
    "\n",
    "                if i % log_per_batches == 0:  # print every `log_per_batches` batches\n",
    "                    avg_train_loss = running_train_loss / log_per_batches  # average the loss \n",
    "                    train_bar.set_description(\"Training Loss: %.3f\" % avg_train_loss)  # update the description of the tqdm progress bar \n",
    "                    running_train_loss = 0  # reset running loss to zero\n",
    "\n",
    "            # iterate over validation data\n",
    "            if val is not None:\n",
    "                val_bar = tqdm(enumerate(valloader, 1), total=len(valloader))  # wrap with tqdm to make a pretty progress bar\n",
    "                with torch.no_grad():  # use `torch.no_grad()` so information about our val set does not get backpropgated\n",
    "                    for i, data in val_bar:\n",
    "                        x, targets = data\n",
    "\n",
    "                        # everything below is the same as the training loop, but notice that we don't need to backprop loss/update optimizer\n",
    "                        predictions = self.model(x)\n",
    "                        loss = loss_fn(predictions, targets)\n",
    "                        running_val_loss += loss\n",
    "                    # average over the entire validation sett\n",
    "                    avg_val_loss = running_val_loss / len(valloader)\n",
    "                    val_bar.set_description(\"Validation Loss: %.3f\" % avg_val_loss)\n",
    "\n",
    "                #set an early stopping condition\n",
    "                # there's a variety of other conditions we can use, but this is nice and simple\n",
    "                if avg_val_loss > prev_val_loss:  # model is still improving!\n",
    "                    state = {\"state_dict\": self.model.state_dict()}\n",
    "                    if self.save_dir is not None:\n",
    "                        try:\n",
    "                            os.makedirs(save_dir)  # make save_dir if save_dir does not already exist\n",
    "                        except FileExistsError:  # save_dir already exists\n",
    "                            pass\n",
    "\n",
    "                        # save path will be `save_dir/run_id/epoch_{epoch_number}`\n",
    "                        save_path = os.path.join(self.save_dir(os.path.join(run_id, \"epoch_\"+str(epoch))))  \n",
    "                        torch.save(state, save_path) # save model\n",
    "\n",
    "                else:  # model is beginning to overfit\n",
    "                    return  # so we stop training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and trainer\n",
    "model = Model()\n",
    "trainer = Trainer(model=model, save_dir=\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishtisehgal/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \n",
      "  0%|          | 2/1875 [00:00<01:36, 19.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.423: 100%|██████████| 1875/1875 [00:42<00:00, 38.91it/s]\n",
      "  0%|          | 4/1875 [00:00<00:48, 38.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.532: 100%|██████████| 1875/1875 [00:51<00:00, 36.22it/s]\n",
      "  0%|          | 4/1875 [00:00<00:52, 35.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.632: 100%|██████████| 1875/1875 [00:54<00:00, 34.52it/s]\n",
      "  0%|          | 4/1875 [00:00<00:52, 35.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.512: 100%|██████████| 1875/1875 [00:33<00:00, 55.24it/s]  \n",
      "  0%|          | 4/1875 [00:00<00:55, 33.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.500: 100%|██████████| 1875/1875 [01:00<00:00, 29.06it/s]\n",
      "  0%|          | 3/1875 [00:00<01:03, 29.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.458: 100%|██████████| 1875/1875 [01:05<00:00, 28.48it/s]\n",
      "  0%|          | 4/1875 [00:00<01:01, 30.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.391: 100%|██████████| 1875/1875 [01:05<00:00, 28.73it/s]\n",
      "  0%|          | 3/1875 [00:00<01:17, 24.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.417: 100%|██████████| 1875/1875 [01:07<00:00, 27.96it/s]\n",
      "  0%|          | 3/1875 [00:00<01:15, 24.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.579: 100%|██████████| 1875/1875 [01:08<00:00, 27.28it/s]\n",
      "  0%|          | 3/1875 [00:00<01:09, 26.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.501: 100%|██████████| 1875/1875 [01:10<00:00, 26.57it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train=mnist, val=None, \n",
    "              epochs=10, batch_size=32, \n",
    "              log_per_batches=10, learning_rate=0.001, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#documentation\n",
    "https://pytorch.org/docs/stable/nn.html\n",
    "#tutorials\n",
    "https://pytorch.org/tutorials/\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "#forums run by the people that crafted PyTorch\n",
    "https://discuss.pytorch.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
